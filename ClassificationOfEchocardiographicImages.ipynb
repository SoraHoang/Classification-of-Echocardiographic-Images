{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClassificationOfEchocardiographicImages.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1vh_dktQjqpZPby2oouBgeOfzPapBstMz",
      "authorship_tag": "ABX9TyNoxeOcFQwJ9mIRYbCjM67x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoraHoang/Classification-of-Echocardiographic-Images/blob/main/ClassificationOfEchocardiographicImages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR196z4uu2bP"
      },
      "source": [
        "# **Thêm các thư viện**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00fDaWl2vB-M"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcQxaAE4vZv8"
      },
      "source": [
        "# **Sử dụng data đã được lưu trên google drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYk7OCBzvm1-"
      },
      "source": [
        "train_path = './drive/MyDrive/Data/DATA_CHAMBER_2021/train'\n",
        "test_path = './drive/MyDrive/Data/DATA_CHAMBER_2021/test'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wAcwN8GwIS-"
      },
      "source": [
        "# **Chuẩn bị dữ liệu**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GyuC8VaUne2"
      },
      "source": [
        "* Tạo namedtuple TrainTest cho tiện thao tác"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LfsOlQfUwNT"
      },
      "source": [
        "TrainTest = namedtuple('TrainTest', ['train', 'test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eXZh3y0U2X9"
      },
      "source": [
        "* 3 classes: {2C, 3C, 4C}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIc7f09vU7lU"
      },
      "source": [
        "def get_classes():\n",
        "  classes = ['2C', '3C', '4C']\n",
        "  return classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds3z8GwpVAja"
      },
      "source": [
        "Chuẩn bị dữ liệu\n",
        "\n",
        "* Đọc dữ liệu từ đường dẫn train_path và test_path\n",
        "* Kích thước không đồng bộ => ảnh cần rescale 224x224x3\n",
        "* Đưa dữ liệu ảnh về dạng tensor => 3x224x224"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxid_TAlVude"
      },
      "source": [
        "def prepare_data():\n",
        "  image_resize = 224\n",
        "\n",
        "  transform_train = transforms.Compose([\n",
        "    transforms.Resize((image_resize, image_resize)),\n",
        "    transforms.ToTensor()\n",
        "  ])\n",
        "  transform_test = transforms.Compose([\n",
        "    transforms.Resize((image_resize, image_resize)),\n",
        "    transforms.ToTensor()\n",
        "  ])\n",
        "\n",
        "  trainset = torchvision.datasets.ImageFolder(root=train_path, transform=transform_train)\n",
        "  testset = torchvision.datasets.ImageFolder(root=test_path, transform=transform_test)\n",
        "\n",
        "  return TrainTest(train=trainset, test=testset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ka3f6A74-2p"
      },
      "source": [
        "Chuẩn bị loader để chia batch dữ liệu\n",
        "\n",
        "*   batch_size = 32\n",
        "*   num_workers = 4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwwTrg00wo_c"
      },
      "source": [
        "def prepare_loader(datasets):\n",
        "  batch_size = 32\n",
        "  num_workers = 4\n",
        "\n",
        "  trainloader = DataLoader(dataset=datasets.train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "  testloader = DataLoader(dataset=datasets.test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "  return TrainTest(train=trainloader, test = testloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-YQ1XfQXxdC"
      },
      "source": [
        "# **Xử lý các epoch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLyrFseAYGzW"
      },
      "source": [
        "Train trong mỗi epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE5cqJXVYEy1"
      },
      "source": [
        "def train_epoch(epoch, model, loader, loss_func, optimizer, device):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  reporting_steps = 60\n",
        "  for i, (images, labels) in enumerate(loader):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    outputs = model(images)\n",
        "    loss = loss_func(outputs, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % reporting_steps == reporting_steps-1:\n",
        "      print(f\"Epoch {epoch} step {i} ave_loss {running_loss/reporting_steps:.4f}\")\n",
        "      running_loss = 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7vXHt68ZH6Y"
      },
      "source": [
        "Test trong mỗi epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjwBLhoGZKoh"
      },
      "source": [
        "def test_epoch(epoch, model, loader, device):\n",
        "  ytrue = []\n",
        "  ypred = []\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    \n",
        "    for i, (images, labels) in enumerate(loader):\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs, dim=1)\n",
        "\n",
        "      ytrue += list(labels.cpu().numpy())\n",
        "      ypred += list(predicted.cpu().numpy())\n",
        "\n",
        "  return ypred, ytrue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn95WiX2Wj3G"
      },
      "source": [
        "# **Xây dựng và thực nghiệm mô hình**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AcENF0aClGv"
      },
      "source": [
        "def main(PATH='./model.pth', MODEL=None):\n",
        "  classes = get_classes()\n",
        "  datasets = prepare_data()\n",
        "  # print('train', len(datasets.train), 'test', len(datasets.test))\n",
        "  # img, label = datasets.train[0]\n",
        "  # print(img.shape)\n",
        "\n",
        "  loaders = prepare_loader(datasets)\n",
        "  device = torch.device(\"cuda:0\")\n",
        "\n",
        "  if MODEL == 'vgg16':\n",
        "    model = torchvision.models.vgg16()\n",
        "    model.classifier[6] = torch.nn.modules.linear.Linear(in_features=4096, out_features=3, bias=True)\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  model.to(device)\n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "  for epoch in range(10):\n",
        "    train_epoch(epoch, model, loaders.train, loss_func, optimizer, device)\n",
        "    ypred, ytrue = test_epoch(epoch, model, loaders.test, device)\n",
        "    print(classification_report(ytrue, ypred, target_names=classes))\n",
        "\n",
        "    torch.save(model.state_dict(), PATH)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpPycYQ4gVgl"
      },
      "source": [
        "Thử mô hình VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j44zkk4SgdUV",
        "outputId": "31a8ac60-b55d-4c9f-ce85-88a8fb4d6769"
      },
      "source": [
        "model = main(PATH='./vgg16.pth', MODEL='vgg16')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 step 59 ave_loss 0.9768\n",
            "Epoch 0 step 119 ave_loss 0.6065\n",
            "Epoch 0 step 179 ave_loss 0.4853\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.76      0.81      0.78       409\n",
            "          3C       0.65      0.94      0.77       367\n",
            "          4C       1.00      0.77      0.87       831\n",
            "\n",
            "    accuracy                           0.82      1607\n",
            "   macro avg       0.80      0.84      0.81      1607\n",
            "weighted avg       0.86      0.82      0.82      1607\n",
            "\n",
            "Epoch 1 step 59 ave_loss 0.2033\n",
            "Epoch 1 step 119 ave_loss 0.1023\n",
            "Epoch 1 step 179 ave_loss 0.0554\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.94      0.85      0.89       409\n",
            "          3C       0.85      0.98      0.91       367\n",
            "          4C       1.00      0.97      0.99       831\n",
            "\n",
            "    accuracy                           0.94      1607\n",
            "   macro avg       0.93      0.94      0.93      1607\n",
            "weighted avg       0.95      0.94      0.94      1607\n",
            "\n",
            "Epoch 2 step 59 ave_loss 0.0257\n",
            "Epoch 2 step 119 ave_loss 0.0516\n",
            "Epoch 2 step 179 ave_loss 0.0322\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.94      0.96      0.95       409\n",
            "          3C       0.91      0.97      0.94       367\n",
            "          4C       1.00      0.96      0.98       831\n",
            "\n",
            "    accuracy                           0.96      1607\n",
            "   macro avg       0.95      0.96      0.96      1607\n",
            "weighted avg       0.97      0.96      0.96      1607\n",
            "\n",
            "Epoch 3 step 59 ave_loss 0.0128\n",
            "Epoch 3 step 119 ave_loss 0.0057\n",
            "Epoch 3 step 179 ave_loss 0.0085\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.68      0.87      0.76       409\n",
            "          3C       0.83      0.90      0.86       367\n",
            "          4C       1.00      0.82      0.90       831\n",
            "\n",
            "    accuracy                           0.85      1607\n",
            "   macro avg       0.84      0.87      0.84      1607\n",
            "weighted avg       0.88      0.85      0.86      1607\n",
            "\n",
            "Epoch 4 step 59 ave_loss 0.0357\n",
            "Epoch 4 step 119 ave_loss 0.0484\n",
            "Epoch 4 step 179 ave_loss 0.0968\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.94      0.95      0.95       409\n",
            "          3C       0.70      0.98      0.82       367\n",
            "          4C       0.99      0.82      0.90       831\n",
            "\n",
            "    accuracy                           0.89      1607\n",
            "   macro avg       0.88      0.92      0.89      1607\n",
            "weighted avg       0.92      0.89      0.89      1607\n",
            "\n",
            "Epoch 5 step 59 ave_loss 0.0019\n",
            "Epoch 5 step 119 ave_loss 0.0004\n",
            "Epoch 5 step 179 ave_loss 0.0001\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.91      0.94      0.93       409\n",
            "          3C       0.72      0.98      0.83       367\n",
            "          4C       1.00      0.82      0.90       831\n",
            "\n",
            "    accuracy                           0.89      1607\n",
            "   macro avg       0.88      0.92      0.89      1607\n",
            "weighted avg       0.91      0.89      0.89      1607\n",
            "\n",
            "Epoch 6 step 59 ave_loss 0.0001\n",
            "Epoch 6 step 119 ave_loss 0.0296\n",
            "Epoch 6 step 179 ave_loss 0.0035\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.83      0.96      0.89       409\n",
            "          3C       0.90      0.98      0.94       367\n",
            "          4C       1.00      0.88      0.93       831\n",
            "\n",
            "    accuracy                           0.92      1607\n",
            "   macro avg       0.91      0.94      0.92      1607\n",
            "weighted avg       0.93      0.92      0.92      1607\n",
            "\n",
            "Epoch 7 step 59 ave_loss 0.0274\n",
            "Epoch 7 step 119 ave_loss 0.0037\n",
            "Epoch 7 step 179 ave_loss 0.0095\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.82      0.81      0.82       409\n",
            "          3C       0.72      1.00      0.84       367\n",
            "          4C       0.98      0.82      0.90       831\n",
            "\n",
            "    accuracy                           0.86      1607\n",
            "   macro avg       0.84      0.88      0.85      1607\n",
            "weighted avg       0.88      0.86      0.86      1607\n",
            "\n",
            "Epoch 8 step 59 ave_loss 0.0203\n",
            "Epoch 8 step 119 ave_loss 0.0201\n",
            "Epoch 8 step 179 ave_loss 0.0033\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.95      0.83      0.89       409\n",
            "          3C       0.67      1.00      0.80       367\n",
            "          4C       1.00      0.84      0.91       831\n",
            "\n",
            "    accuracy                           0.87      1607\n",
            "   macro avg       0.87      0.89      0.87      1607\n",
            "weighted avg       0.91      0.87      0.88      1607\n",
            "\n",
            "Epoch 9 step 59 ave_loss 0.0002\n",
            "Epoch 9 step 119 ave_loss 0.0002\n",
            "Epoch 9 step 179 ave_loss 0.0000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.92      0.91      0.91       409\n",
            "          3C       0.70      0.97      0.81       367\n",
            "          4C       1.00      0.83      0.91       831\n",
            "\n",
            "    accuracy                           0.88      1607\n",
            "   macro avg       0.87      0.90      0.88      1607\n",
            "weighted avg       0.91      0.88      0.89      1607\n",
            "\n"
          ]
        }
      ]
    }
  ]
}