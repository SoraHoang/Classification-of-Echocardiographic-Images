{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ClassificationOfEchocardiographicImages.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoraHoang/Classification-of-Echocardiographic-Images/blob/main/ClassificationOfEchocardiographicImages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR196z4uu2bP"
      },
      "source": [
        "# **Thêm các thư viện**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00fDaWl2vB-M"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcQxaAE4vZv8"
      },
      "source": [
        "# **Sử dụng data đã được lưu trên google drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYk7OCBzvm1-"
      },
      "source": [
        "train_path = './drive/MyDrive/Data/DATA_CHAMBER_2021/train'\n",
        "test_path = './drive/MyDrive/Data/DATA_CHAMBER_2021/test'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wAcwN8GwIS-"
      },
      "source": [
        "# **Chuẩn bị dữ liệu**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GyuC8VaUne2"
      },
      "source": [
        "* Tạo namedtuple TrainTest cho tiện thao tác"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LfsOlQfUwNT"
      },
      "source": [
        "TrainTest = namedtuple('TrainTest', ['train', 'test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eXZh3y0U2X9"
      },
      "source": [
        "* 3 classes: {2C, 3C, 4C}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIc7f09vU7lU"
      },
      "source": [
        "def get_classes():\n",
        "  classes = ['2C', '3C', '4C']\n",
        "  return classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds3z8GwpVAja"
      },
      "source": [
        "Chuẩn bị dữ liệu\n",
        "\n",
        "* Đọc dữ liệu từ đường dẫn train_path và test_path\n",
        "* Kích thước không đồng bộ => ảnh raw cần rescale 224x224x3 (các ảnh scale sẽ có kích thước nhỏ hơn)\n",
        "* Đưa dữ liệu ảnh về dạng tensor => 3ximage_resizeximage_resize\n",
        "* Tùy theo các xử lý dữ liệu mà sẽ cho preprocess hoặc augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxid_TAlVude"
      },
      "source": [
        "def choose_transform(image_resize=224, input_type='normal'):\n",
        "  # size = {224, 64, 32}\n",
        "  # type = {'normal', 'preprocess', 'augmentation'}\n",
        "  if input_type == 'normal':\n",
        "    transform_train = transforms.Compose([\n",
        "      transforms.Resize((image_resize, image_resize)),\n",
        "      transforms.ToTensor()\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "      transforms.Resize((image_resize, image_resize)),\n",
        "      transforms.ToTensor()\n",
        "    ])\n",
        "  elif input_type == 'preprocess':\n",
        "    transform_train = transforms.Compose([\n",
        "      transforms.Resize((image_resize, image_resize)),\n",
        "      transforms.RandomEqualize(p=1),\n",
        "      transforms.GaussianBlur(kernel_size=3),\n",
        "      transforms.ToTensor()\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "      transforms.Resize((image_resize, image_resize)),\n",
        "      transforms.RandomEqualize(p=1),\n",
        "      transforms.GaussianBlur(kernel_size=3),\n",
        "      transforms.ToTensor()\n",
        "    ])\n",
        "  elif input_type == 'augmentation':\n",
        "    transform_train = transforms.Compose([\n",
        "      transforms.Resize((image_resize, image_resize)),\n",
        "      transforms.RandomHorizontalFlip(p=0.3),\n",
        "      transforms.RandomVerticalFlip(p=0.3),\n",
        "      transforms.RandomRotation(degrees=10),\n",
        "      transforms.ToTensor()\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "      transforms.Resize((image_resize, image_resize)),\n",
        "      transforms.ToTensor()\n",
        "    ])\n",
        "  return transform_train, transform_test\n",
        "\n",
        "def prepare_data(transform_train, transform_test):\n",
        "  trainset = torchvision.datasets.ImageFolder(root=train_path, transform=transform_train)\n",
        "  testset = torchvision.datasets.ImageFolder(root=test_path, transform=transform_test)\n",
        "\n",
        "  return TrainTest(train=trainset, test=testset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ka3f6A74-2p"
      },
      "source": [
        "Chuẩn bị loader để chia batch dữ liệu\n",
        "\n",
        "*   batch_size = 32\n",
        "*   num_workers = 4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwwTrg00wo_c"
      },
      "source": [
        "def prepare_loader(datasets):\n",
        "  batch_size = 32\n",
        "  num_workers = 4\n",
        "\n",
        "  trainloader = DataLoader(dataset=datasets.train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "  testloader = DataLoader(dataset=datasets.test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "  return TrainTest(train=trainloader, test = testloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-YQ1XfQXxdC"
      },
      "source": [
        "# **Xử lý các epoch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLyrFseAYGzW"
      },
      "source": [
        "Train trong mỗi epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE5cqJXVYEy1"
      },
      "source": [
        "def train_epoch(epoch, model, loader, loss_func, optimizer, device):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  final_loss = 0.0\n",
        "  reporting_steps = 60\n",
        "  for i, (images, labels) in enumerate(loader):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    outputs = model(images)\n",
        "    loss = loss_func(outputs, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    final_loss = loss.item()\n",
        "    running_loss += final_loss\n",
        "    if i % reporting_steps == reporting_steps - 1:\n",
        "      print(f\"Epoch {epoch} step {i} ave_loss {running_loss/reporting_steps:.4f}\")\n",
        "      running_loss = 0.0\n",
        "  return final_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7vXHt68ZH6Y"
      },
      "source": [
        "Test trong mỗi epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjwBLhoGZKoh"
      },
      "source": [
        "def test_epoch(epoch, model, loader, device):\n",
        "  ytrue = []\n",
        "  ypred = []\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    \n",
        "    for i, (images, labels) in enumerate(loader):\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs, dim=1)\n",
        "\n",
        "      ytrue += list(labels.cpu().numpy())\n",
        "      ypred += list(predicted.cpu().numpy())\n",
        "\n",
        "  return ypred, ytrue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn95WiX2Wj3G"
      },
      "source": [
        "# **Xây dựng và thực nghiệm mô hình**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AcENF0aClGv"
      },
      "source": [
        "def main(MODEL, image_resize, input_type):\n",
        "  PATH = './' + MODEL + '_' + str(image_resize) + '_' + input_type + '.pth'\n",
        "  classes = get_classes()\n",
        "  num_classes = len(classes)\n",
        "\n",
        "  transform_train, transform_test = choose_transform(image_resize, input_type)\n",
        "  datasets = prepare_data(transform_train, transform_test)\n",
        "\n",
        "  loaders = prepare_loader(datasets)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  if MODEL == 'vgg16':\n",
        "    model = torchvision.models.vgg16()\n",
        "\n",
        "    num_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = torch.nn.modules.linear.Linear(in_features=num_features, out_features=num_classes)\n",
        "  elif MODEL == 'resnet50':\n",
        "    model = torchvision.models.resnet50()\n",
        "\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = torch.nn.modules.linear.Linear(in_features=num_features, out_features=num_classes)\n",
        "  elif MODEL == 'densenet121':\n",
        "    model = torchvision.models.densenet121()\n",
        "\n",
        "    num_features = model.classifier.in_features\n",
        "    model.classifier = torch.nn.modules.linear.Linear(in_features=num_features, out_features=num_classes)\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  model.to(device)\n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "  accuracies = []\n",
        "  losses = []\n",
        "  for epoch in range(10):\n",
        "    loss = train_epoch(epoch, model, loaders.train, loss_func, optimizer, device)\n",
        "    losses.append(round(loss, 4))\n",
        "\n",
        "    ypred, ytrue = test_epoch(epoch, model, loaders.test, device)\n",
        "    print(classification_report(ytrue, ypred, target_names=classes))\n",
        "\n",
        "    torch.save(model.state_dict(), PATH)\n",
        "\n",
        "    ypred = np.array(ypred)\n",
        "    ytrue = np.array(ytrue)\n",
        "\n",
        "    accuracy = int((ytrue == ypred).sum() / len(ytrue) * 100)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "  print(\"model:\", MODEL, \", size:\", image_resize, \", type:\", input_type)\n",
        "  print(\"accuracy: \", accuracies)\n",
        "  print(\"loss: \", losses)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpPycYQ4gVgl"
      },
      "source": [
        "# Thử mô hình"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQHIIcAHRH9r"
      },
      "source": [
        "Test mô hình đã huấn luyện trên video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpYWchOwRNs8"
      },
      "source": [
        "def video_report(model, testsets):\n",
        "  video = namedtuple('video', ['id', \"label_true\", 'label_pred'])\n",
        "  path = []\n",
        "\n",
        "  for index, image in enumerate(testsets.imgs):\n",
        "    path.append(image[0].split(\"/\")[-1].split(\"_\")[0])\n",
        "  \n",
        "  vid_list = []\n",
        "  for frame in path:\n",
        "    if (frame in vid_list) == False:\n",
        "      vid_list.append(frame)\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  ytrue = []\n",
        "  ypred = []\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    for image, label in testsets:\n",
        "      image = image.unsqueeze(0).to(device)\n",
        "      output = model(image)\n",
        "      _, predicted = torch.max(output, dim=1)\n",
        "      ytrue.append(label)\n",
        "      ypred += list(predicted.cpu().numpy())\n",
        "\n",
        "  outputs_vid = []\n",
        "  ytrue_video = []\n",
        "  ypred_video = []\n",
        "  for vid_id in vid_list:\n",
        "    vid_true = []\n",
        "    vid_pred = []\n",
        "    for index, img in enumerate(path):\n",
        "      if img == vid_id:\n",
        "        vid_true.append(ytrue[index])\n",
        "        vid_pred.append(ypred[index])\n",
        "\n",
        "    rate_video_true = [0, 0, 0] # vector count for voting\n",
        "    rate_video_pred = [0, 0, 0]\n",
        "    for label in range(3):\n",
        "      rate_video_true[label] = list(vid_true).count(label)\n",
        "      rate_video_pred[label] = list(vid_pred).count(label)\n",
        "    \n",
        "    label_video_true = np.argmax(rate_video_true)\n",
        "    label_video_pred = np.argmax(rate_video_pred)\n",
        "\n",
        "    print(\"id:\", vid_id, \", true:\", label_video_true, \", pred:\",label_video_pred) # label pred\n",
        "    ytrue_video.append(label_video_true)\n",
        "    ypred_video.append(label_video_pred)\n",
        "    outputs_vid.append(video(id=vid_id, label_true=label_video_true, label_pred=label_video_pred))\n",
        "  print(classification_report(ytrue_video, ypred_video, target_names=get_classes()))\n",
        "  return outputs_vid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j44zkk4SgdUV"
      },
      "source": [
        "models = ['vgg16', 'resnet50', 'densenet121']\n",
        "sizes = [224, 64, 32]\n",
        "input_types = ['normal', 'preprocess', 'augmentation']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTYbnRbLJw03",
        "outputId": "9f75b94e-7e70-49d0-b9e6-0de9bcf20725"
      },
      "source": [
        "model = main('densenet121', 224, 'augmentation')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 step 59 ave_loss 1.0597\n",
            "Epoch 0 step 119 ave_loss 0.7584\n",
            "Epoch 0 step 179 ave_loss 0.6047\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.68      0.81      0.74       409\n",
            "          3C       0.48      0.77      0.59       367\n",
            "          4C       1.00      0.63      0.77       831\n",
            "\n",
            "    accuracy                           0.71      1607\n",
            "   macro avg       0.72      0.74      0.70      1607\n",
            "weighted avg       0.80      0.71      0.72      1607\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 step 59 ave_loss 0.3428\n",
            "Epoch 1 step 119 ave_loss 0.2713\n",
            "Epoch 1 step 179 ave_loss 0.2698\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.65      0.71      0.68       409\n",
            "          3C       0.40      0.87      0.55       367\n",
            "          4C       0.91      0.39      0.55       831\n",
            "\n",
            "    accuracy                           0.58      1607\n",
            "   macro avg       0.65      0.66      0.59      1607\n",
            "weighted avg       0.73      0.58      0.58      1607\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 step 59 ave_loss 0.1338\n",
            "Epoch 2 step 119 ave_loss 0.1129\n",
            "Epoch 2 step 179 ave_loss 0.1301\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.56      0.68      0.62       409\n",
            "          3C       0.61      0.80      0.69       367\n",
            "          4C       0.91      0.69      0.78       831\n",
            "\n",
            "    accuracy                           0.71      1607\n",
            "   macro avg       0.69      0.72      0.70      1607\n",
            "weighted avg       0.75      0.71      0.72      1607\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 step 59 ave_loss 0.0849\n",
            "Epoch 3 step 119 ave_loss 0.1017\n",
            "Epoch 3 step 179 ave_loss 0.0662\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.63      0.81      0.71       409\n",
            "          3C       0.33      0.85      0.48       367\n",
            "          4C       1.00      0.17      0.29       831\n",
            "\n",
            "    accuracy                           0.49      1607\n",
            "   macro avg       0.65      0.61      0.49      1607\n",
            "weighted avg       0.75      0.49      0.44      1607\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 step 59 ave_loss 0.0617\n",
            "Epoch 4 step 119 ave_loss 0.0582\n",
            "Epoch 4 step 179 ave_loss 0.0448\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.57      0.82      0.67       409\n",
            "          3C       0.51      0.85      0.64       367\n",
            "          4C       1.00      0.49      0.66       831\n",
            "\n",
            "    accuracy                           0.66      1607\n",
            "   macro avg       0.70      0.72      0.66      1607\n",
            "weighted avg       0.78      0.66      0.66      1607\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 step 59 ave_loss 0.0348\n",
            "Epoch 5 step 119 ave_loss 0.0311\n",
            "Epoch 5 step 179 ave_loss 0.0393\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.79      0.79      0.79       409\n",
            "          3C       0.56      0.83      0.67       367\n",
            "          4C       1.00      0.79      0.88       831\n",
            "\n",
            "    accuracy                           0.80      1607\n",
            "   macro avg       0.78      0.80      0.78      1607\n",
            "weighted avg       0.85      0.80      0.81      1607\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 step 59 ave_loss 0.0267\n",
            "Epoch 6 step 119 ave_loss 0.0561\n",
            "Epoch 6 step 179 ave_loss 0.0322\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.70      0.76      0.73       409\n",
            "          3C       0.65      0.88      0.75       367\n",
            "          4C       0.97      0.77      0.86       831\n",
            "\n",
            "    accuracy                           0.79      1607\n",
            "   macro avg       0.77      0.81      0.78      1607\n",
            "weighted avg       0.83      0.79      0.80      1607\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 step 59 ave_loss 0.0322\n",
            "Epoch 7 step 119 ave_loss 0.0399\n",
            "Epoch 7 step 179 ave_loss 0.0162\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.66      0.81      0.73       409\n",
            "          3C       0.68      0.88      0.76       367\n",
            "          4C       1.00      0.76      0.86       831\n",
            "\n",
            "    accuracy                           0.80      1607\n",
            "   macro avg       0.78      0.82      0.79      1607\n",
            "weighted avg       0.84      0.80      0.81      1607\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 step 59 ave_loss 0.0186\n",
            "Epoch 8 step 119 ave_loss 0.0197\n",
            "Epoch 8 step 179 ave_loss 0.0216\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.63      0.82      0.71       409\n",
            "          3C       0.63      0.87      0.73       367\n",
            "          4C       1.00      0.69      0.81       831\n",
            "\n",
            "    accuracy                           0.76      1607\n",
            "   macro avg       0.75      0.79      0.75      1607\n",
            "weighted avg       0.82      0.76      0.77      1607\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 step 59 ave_loss 0.0201\n",
            "Epoch 9 step 119 ave_loss 0.0177\n",
            "Epoch 9 step 179 ave_loss 0.0096\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.58      0.80      0.67       409\n",
            "          3C       0.63      0.81      0.71       367\n",
            "          4C       0.96      0.65      0.77       831\n",
            "\n",
            "    accuracy                           0.72      1607\n",
            "   macro avg       0.72      0.75      0.72      1607\n",
            "weighted avg       0.79      0.72      0.73      1607\n",
            "\n",
            "model: densenet121 , size: 224 , type: augmentation\n",
            "accuracy:  [71, 58, 71, 48, 65, 79, 79, 79, 76, 72]\n",
            "loss:  [0.3119, 0.2021, 0.0685, 0.0082, 0.1915, 0.0088, 0.0469, 0.1097, 0.0021, 0.0011]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaMMvD4lTdiv",
        "outputId": "d5f4b55b-51ea-4175-bea9-d19d32296be7"
      },
      "source": [
        "test_transform = transforms.Compose([\n",
        "  transforms.Resize((224, 224)),\n",
        "  # transforms.RandomEqualize(p=1),\n",
        "  # transforms.GaussianBlur(kernel_size=3),\n",
        "  transforms.ToTensor()\n",
        "])\n",
        "\n",
        "testsets = torchvision.datasets.ImageFolder(root=test_path, transform=test_transform)\n",
        "video_report(model=model, testsets=testsets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id: 158 , true: 0 , pred: 1\n",
            "id: 165 , true: 0 , pred: 0\n",
            "id: 168 , true: 0 , pred: 1\n",
            "id: 169 , true: 0 , pred: 1\n",
            "id: 171 , true: 0 , pred: 2\n",
            "id: 176 , true: 0 , pred: 0\n",
            "id: 177 , true: 0 , pred: 1\n",
            "id: 178 , true: 0 , pred: 0\n",
            "id: 181 , true: 0 , pred: 0\n",
            "id: 183 , true: 0 , pred: 1\n",
            "id: 191 , true: 0 , pred: 0\n",
            "id: 192 , true: 0 , pred: 0\n",
            "id: 157 , true: 1 , pred: 1\n",
            "id: 159 , true: 1 , pred: 1\n",
            "id: 161 , true: 1 , pred: 1\n",
            "id: 162 , true: 1 , pred: 1\n",
            "id: 166 , true: 1 , pred: 1\n",
            "id: 174 , true: 1 , pred: 1\n",
            "id: 175 , true: 1 , pred: 1\n",
            "id: 179 , true: 1 , pred: 0\n",
            "id: 185 , true: 1 , pred: 1\n",
            "id: 186 , true: 1 , pred: 1\n",
            "id: 189 , true: 1 , pred: 1\n",
            "id: 190 , true: 1 , pred: 1\n",
            "id: 194 , true: 1 , pred: 1\n",
            "id: 160 , true: 2 , pred: 2\n",
            "id: 163 , true: 2 , pred: 2\n",
            "id: 164 , true: 2 , pred: 2\n",
            "id: 167 , true: 2 , pred: 2\n",
            "id: 170 , true: 2 , pred: 2\n",
            "id: 172 , true: 2 , pred: 2\n",
            "id: 173 , true: 2 , pred: 2\n",
            "id: 180 , true: 2 , pred: 2\n",
            "id: 182 , true: 2 , pred: 2\n",
            "id: 184 , true: 2 , pred: 2\n",
            "id: 187 , true: 2 , pred: 2\n",
            "id: 188 , true: 2 , pred: 2\n",
            "id: 193 , true: 2 , pred: 2\n",
            "id: 195 , true: 2 , pred: 2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          2C       0.86      0.50      0.63        12\n",
            "          3C       0.71      0.92      0.80        13\n",
            "          4C       0.93      1.00      0.97        14\n",
            "\n",
            "    accuracy                           0.82        39\n",
            "   macro avg       0.83      0.81      0.80        39\n",
            "weighted avg       0.83      0.82      0.81        39\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[video(id='158', label_true=0, label_pred=1),\n",
              " video(id='165', label_true=0, label_pred=0),\n",
              " video(id='168', label_true=0, label_pred=1),\n",
              " video(id='169', label_true=0, label_pred=1),\n",
              " video(id='171', label_true=0, label_pred=2),\n",
              " video(id='176', label_true=0, label_pred=0),\n",
              " video(id='177', label_true=0, label_pred=1),\n",
              " video(id='178', label_true=0, label_pred=0),\n",
              " video(id='181', label_true=0, label_pred=0),\n",
              " video(id='183', label_true=0, label_pred=1),\n",
              " video(id='191', label_true=0, label_pred=0),\n",
              " video(id='192', label_true=0, label_pred=0),\n",
              " video(id='157', label_true=1, label_pred=1),\n",
              " video(id='159', label_true=1, label_pred=1),\n",
              " video(id='161', label_true=1, label_pred=1),\n",
              " video(id='162', label_true=1, label_pred=1),\n",
              " video(id='166', label_true=1, label_pred=1),\n",
              " video(id='174', label_true=1, label_pred=1),\n",
              " video(id='175', label_true=1, label_pred=1),\n",
              " video(id='179', label_true=1, label_pred=0),\n",
              " video(id='185', label_true=1, label_pred=1),\n",
              " video(id='186', label_true=1, label_pred=1),\n",
              " video(id='189', label_true=1, label_pred=1),\n",
              " video(id='190', label_true=1, label_pred=1),\n",
              " video(id='194', label_true=1, label_pred=1),\n",
              " video(id='160', label_true=2, label_pred=2),\n",
              " video(id='163', label_true=2, label_pred=2),\n",
              " video(id='164', label_true=2, label_pred=2),\n",
              " video(id='167', label_true=2, label_pred=2),\n",
              " video(id='170', label_true=2, label_pred=2),\n",
              " video(id='172', label_true=2, label_pred=2),\n",
              " video(id='173', label_true=2, label_pred=2),\n",
              " video(id='180', label_true=2, label_pred=2),\n",
              " video(id='182', label_true=2, label_pred=2),\n",
              " video(id='184', label_true=2, label_pred=2),\n",
              " video(id='187', label_true=2, label_pred=2),\n",
              " video(id='188', label_true=2, label_pred=2),\n",
              " video(id='193', label_true=2, label_pred=2),\n",
              " video(id='195', label_true=2, label_pred=2)]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    }
  ]
}